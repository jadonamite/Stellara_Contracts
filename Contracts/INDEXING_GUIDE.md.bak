# Event Indexing Infrastructure Guide

## Overview

This guide provides comprehensive recommendations for building event indexing infrastructure for Stellara Network contracts. It covers architecture, implementation, and best practices for off-chain event processing.

## Architecture Overview

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Stellar RPC   │───▶│  Event Stream   │───▶│   Event Indexer │
│    Provider     │    │   Processor     │    │     Service     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                                        │
                                                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Analytics     │◀───│   Query API     │◀───│   Time-Series   │
│   Dashboard     │    │   (GraphQL)     │    │   Database      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## Technology Stack Recommendations

### Core Infrastructure

| Component | Recommended Technology | Alternatives |
|-----------|----------------------|--------------|
| Event Streaming | Apache Kafka | NATS, RabbitMQ |
| Database | PostgreSQL + TimescaleDB | ClickHouse, InfluxDB |
| API Layer | GraphQL + REST | REST only |
| Cache | Redis | Memcached |
| Message Queue | Apache Kafka | RabbitMQ, SQS |
| Monitoring | Prometheus + Grafana | Datadog, New Relic |

### Development Stack

| Component | Recommended Technology | Alternatives |
|-----------|----------------------|--------------|
| Language | TypeScript/Node.js | Go, Rust, Python |
| Framework | Express.js + Apollo GraphQL | FastAPI, Gin |
| ORM | Prisma | TypeORM, Sequelize |
| Testing | Jest | Mocha, Pytest |
| Containerization | Docker | Podman |

---

## Implementation Guide

### 1. Event Stream Processor

#### Setup Kafka Topic Configuration

```yaml
# kafka-topics.yml
topics:
  - name: stellar-events
    partitions: 12
    replication-factor: 3
    config:
      retention.ms: 7776000000  # 90 days
      segment.bytes: 1073741824  # 1GB
```

#### Event Processor Implementation (Node.js)

```typescript
// EventProcessor.ts
import { Kafka, Consumer } from 'kafkajs';
import { StellarRPC } from 'stellar-sdk';
import { EventParser } from './EventParser';
import { EventStore } from './EventStore';

export class EventProcessor {
  private kafka: Kafka;
  private consumer: Consumer;
  private stellarRPC: StellarRPC;
  private eventParser: EventParser;
  private eventStore: EventStore;

  constructor() {
    this.kafka = new Kafka({
      clientId: 'stellar-event-processor',
      brokers: process.env.KAFKA_BROKERS?.split(',') || ['localhost:9092']
    });

    this.consumer = this.kafka.consumer({ groupId: 'stellar-indexers' });
    this.stellarRPC = new StellarRPC(process.env.STELLAR_RPC_URL);
    this.eventParser = new EventParser();
    this.eventStore = new EventStore();
  }

  async start() {
    await this.consumer.connect();
    await this.consumer.subscribe({ topic: 'stellar-events' });

    await this.consumer.run({
      eachMessage: async ({ message }) => {
        try {
          const event = JSON.parse(message.value?.toString() || '{}');
          await this.processEvent(event);
        } catch (error) {
          console.error('Error processing event:', error);
          // Implement retry logic
        }
      }
    });
  }

  private async processEvent(rawEvent: any) {
    // Parse Stellar event
    const parsedEvent = this.eventParser.parse(rawEvent);
    
    // Validate event schema
    if (!this.eventParser.validate(parsedEvent)) {
      throw new Error('Invalid event schema');
    }

    // Store in database
    await this.eventStore.store(parsedEvent);
    
    // Update derived metrics
    await this.updateMetrics(parsedEvent);
  }

  private async updateMetrics(event: ParsedEvent) {
    // Update real-time metrics
    switch (event.topic) {
      case 'transfer':
        await this.updateTransferMetrics(event);
        break;
      case 'stake':
        await this.updateStakingMetrics(event);
        break;
      // ... other event types
    }
  }
}
```

### 2. Database Schema Design

#### PostgreSQL Schema

```sql
-- Main events table
CREATE TABLE events (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    ledger_sequence BIGINT NOT NULL,
    contract_address VARCHAR(56) NOT NULL,
    topic VARCHAR(20) NOT NULL,
    event_data JSONB NOT NULL,
    timestamp BIGINT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_events_ledger_sequence ON events(ledger_sequence);
CREATE INDEX idx_events_contract_address ON events(contract_address);
CREATE INDEX idx_events_topic ON events(topic);
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_contract_topic ON events(contract_address, topic);

-- Partitioning by time (TimescaleDB)
SELECT create_hypertable('events', 'timestamp', chunk_time_interval => INTERVAL '1 day');

-- Token-specific table for fast queries
CREATE TABLE token_transfers (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    event_id UUID REFERENCES events(id),
    from_address VARCHAR(56) NOT NULL,
    to_address VARCHAR(56) NOT NULL,
    amount NUMERIC(39,0) NOT NULL,
    token_address VARCHAR(56) NOT NULL,
    timestamp BIGINT NOT NULL
);

CREATE INDEX idx_token_transfers_from ON token_transfers(from_address);
CREATE INDEX idx_token_transfers_to ON token_transfers(to_address);
CREATE INDEX idx_token_transfers_token ON token_transfers(token_address);

-- Staking metrics table
CREATE TABLE staking_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    contract_address VARCHAR(56) NOT NULL,
    total_staked NUMERIC(39,0) NOT NULL,
    total_stakers INTEGER NOT NULL,
    average_lock_period BIGINT NOT NULL,
    timestamp BIGINT NOT NULL
);

SELECT create_hypertable('staking_metrics', 'timestamp', chunk_time_interval => INTERVAL '1 hour');
```

#### Event Store Implementation

```typescript
// EventStore.ts
import { Pool } from 'pg';

export class EventStore {
  private pool: Pool;

  constructor() {
    this.pool = new Pool({
      host: process.env.DB_HOST,
      port: parseInt(process.env.DB_PORT || '5432'),
      database: process.env.DB_NAME,
      user: process.env.DB_USER,
      password: process.env.DB_PASSWORD,
      max: 20
    });
  }

  async store(event: ParsedEvent): Promise<void> {
    const client = await this.pool.connect();
    
    try {
      await client.query('BEGIN');
      
      // Store main event
      const eventResult = await client.query(
        `INSERT INTO events (ledger_sequence, contract_address, topic, event_data, timestamp)
         VALUES ($1, $2, $3, $4, $5)
         RETURNING id`,
        [event.ledgerSequence, event.contractAddress, event.topic, event.data, event.timestamp]
      );

      // Store specialized data based on event type
      await this.storeSpecializedData(client, eventResult.rows[0].id, event);
      
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }

  private async storeSpecializedData(client: any, eventId: string, event: ParsedEvent): Promise<void> {
    switch (event.topic) {
      case 'transfer':
        await this.storeTransferData(client, eventId, event.data);
        break;
      case 'stake':
        await this.storeStakingData(client, eventId, event.data);
        break;
      // ... other event types
    }
  }

  private async storeTransferData(client: any, eventId: string, data: any): Promise<void> {
    await client.query(
      `INSERT INTO token_transfers (event_id, from_address, to_address, amount, token_address, timestamp)
       VALUES ($1, $2, $3, $4, $5, $6)`,
      [eventId, data.from, data.to, data.amount, data.token, data.timestamp]
    );
  }
}
```

### 3. GraphQL API Layer

#### Schema Definition

```graphql
# schema.graphql
type Event {
  id: ID!
  ledgerSequence: BigInt!
  contractAddress: String!
  topic: String!
  data: JSON!
  timestamp: BigInt!
  createdAt: DateTime!
}

type TokenTransfer {
  id: ID!
  from: String!
  to: String!
  amount: BigInt!
  tokenAddress: String!
  timestamp: BigInt!
  event: Event!
}

type StakingMetrics {
  id: ID!
  contractAddress: String!
  totalStaked: BigInt!
  totalStakers: Int!
  averageLockPeriod: BigInt!
  timestamp: BigInt!
}

type Query {
  events(
    contractAddress: String
    topic: String
    fromTimestamp: BigInt
    toTimestamp: BigInt
    limit: Int = 100
    offset: Int = 0
  ): [Event!]!

  tokenTransfers(
    tokenAddress: String!
    fromAddress: String
    toAddress: String
    fromTimestamp: BigInt
    toTimestamp: BigInt
    limit: Int = 100
    offset: Int = 0
  ): [TokenTransfer!]!

  stakingMetrics(
    contractAddress: String!
    fromTimestamp: BigInt
    toTimestamp: BigInt
    limit: Int = 100
  ): [StakingMetrics!]!

  transferVolume(
    tokenAddress: String!
    fromTimestamp: BigInt!
    toTimestamp: BigInt!
  ): BigInt!
}

type Subscription {
  events(contractAddress: String, topic: String): Event!
  tokenTransfers(tokenAddress: String): TokenTransfer!
}
```

#### Resolver Implementation

```typescript
// resolvers.ts
import { PubSub } from 'graphql-subscriptions';

const pubsub = new PubSub();

export const resolvers = {
  Query: {
    events: async (_: any, args: any) => {
      const { contractAddress, topic, fromTimestamp, toTimestamp, limit, offset } = args;
      
      let query = 'SELECT * FROM events WHERE 1=1';
      const params: any[] = [];
      let paramIndex = 1;

      if (contractAddress) {
        query += ` AND contract_address = $${paramIndex++}`;
        params.push(contractAddress);
      }

      if (topic) {
        query += ` AND topic = $${paramIndex++}`;
        params.push(topic);
      }

      if (fromTimestamp) {
        query += ` AND timestamp >= $${paramIndex++}`;
        params.push(fromTimestamp);
      }

      if (toTimestamp) {
        query += ` AND timestamp <= $${paramIndex++}`;
        params.push(toTimestamp);
      }

      query += ` ORDER BY timestamp DESC LIMIT $${paramIndex++} OFFSET $${paramIndex++}`;
      params.push(limit, offset);

      const result = await pool.query(query, params);
      return result.rows;
    },

    transferVolume: async (_: any, { tokenAddress, fromTimestamp, toTimestamp }: any) => {
      const result = await pool.query(
        `SELECT COALESCE(SUM(amount), 0) as volume
         FROM token_transfers
         WHERE token_address = $1 AND timestamp BETWEEN $2 AND $3`,
        [tokenAddress, fromTimestamp, toTimestamp]
      );

      return result.rows[0].volume;
    }
  },

  Subscription: {
    events: {
      subscribe: (_: any, args: any) => {
        const { contractAddress, topic } = args;
        const channelName = contractAddress 
          ? topic 
            ? `EVENT_${contractAddress}_${topic}`
            : `EVENT_${contractAddress}`
          : 'EVENT_ALL';

        return pubsub.asyncIterator([channelName]);
      }
    }
  }
};
```

### 4. Real-time Event Streaming

#### WebSocket Implementation

```typescript
// EventStreamer.ts
import { Server } from 'socket.io';
import { PubSub } from 'graphql-subscriptions';

export class EventStreamer {
  private io: Server;
  private pubsub: PubSub;

  constructor(server: any) {
    this.io = new Server(server, {
      cors: {
        origin: process.env.ALLOWED_ORIGINS?.split(',') || ['http://localhost:3000']
      }
    });
    this.pubsub = new PubSub();
    this.setupEventHandlers();
  }

  private setupEventHandlers() {
    // Subscribe to new events
    this.pubsub.subscribe('EVENT_ALL').subscribe((event) => {
      this.io.emit('event', event);
    });

    // Handle client connections
    this.io.on('connection', (socket) => {
      console.log('Client connected:', socket.id);

      // Handle subscription to specific events
      socket.on('subscribe', (filters) => {
        const { contractAddress, topic } = filters;
        const channelName = contractAddress 
          ? topic 
            ? `EVENT_${contractAddress}_${topic}`
            : `EVENT_${contractAddress}`
          : 'EVENT_ALL';

        this.pubsub.subscribe(channelName).subscribe((event) => {
          socket.emit('event', event);
        });
      });

      socket.on('disconnect', () => {
        console.log('Client disconnected:', socket.id);
      });
    });
  }

  publishEvent(event: ParsedEvent) {
    // Publish to general channel
    this.pubsub.publish('EVENT_ALL', { event });

    // Publish to specific channels
    const contractChannel = `EVENT_${event.contractAddress}`;
    this.pubsub.publish(contractChannel, { event });

    const topicChannel = `EVENT_${event.contractAddress}_${event.topic}`;
    this.pubsub.publish(topicChannel, { event });
  }
}
```

---

## Performance Optimization

### 1. Database Optimization

#### Indexing Strategy

```sql
-- Composite indexes for common queries
CREATE INDEX idx_events_contract_topic_time ON events(contract_address, topic, timestamp DESC);
CREATE INDEX idx_token_transfers_token_time ON token_transfers(token_address, timestamp DESC);

-- Partial indexes for filtering
CREATE INDEX idx_events_recent ON events(timestamp) WHERE timestamp > NOW() - INTERVAL '7 days';
CREATE INDEX idx_large_transfers ON token_transfers(amount) WHERE amount > 1000000;

-- Hash indexes for exact matches
CREATE INDEX idx_events_contract_hash ON events USING HASH(contract_address);
```

#### Query Optimization

```typescript
// Optimized query builder
class QueryBuilder {
  static buildEventQuery(filters: EventFilters): { query: string; params: any[] } {
    const conditions: string[] = [];
    const params: any[] = [];
    let paramIndex = 1;

    // Use CTE for complex queries
    let query = `
      WITH filtered_events AS (
        SELECT *
        FROM events
        WHERE 1=1
    `;

    if (filters.contractAddress) {
      conditions.push(`contract_address = $${paramIndex++}`);
      params.push(filters.contractAddress);
    }

    if (filters.topic) {
      conditions.push(`topic = $${paramIndex++}`);
      params.push(filters.topic);
    }

    if (filters.fromTimestamp) {
      conditions.push(`timestamp >= $${paramIndex++}`);
      params.push(filters.fromTimestamp);
    }

    if (filters.toTimestamp) {
      conditions.push(`timestamp <= $${paramIndex++}`);
      params.push(filters.toTimestamp);
    }

    if (conditions.length > 0) {
      query += ' AND ' + conditions.join(' AND ');
    }

    query += `
        ORDER BY timestamp DESC
        LIMIT $${paramIndex++}
        OFFSET $${paramIndex++}
      )
      SELECT e.*, 
             CASE 
               WHEN e.topic = 'transfer' THEN 
                 (SELECT row_to_json(t) FROM token_transfers t WHERE t.event_id = e.id)
               ELSE NULL 
             END as specialized_data
      FROM filtered_events e
    `;

    params.push(filters.limit || 100, filters.offset || 0);

    return { query, params };
  }
}
```

### 2. Caching Strategy

#### Redis Implementation

```typescript
// CacheManager.ts
import Redis from 'ioredis';

export class CacheManager {
  private redis: Redis;

  constructor() {
    this.redis = new Redis({
      host: process.env.REDIS_HOST,
      port: parseInt(process.env.REDIS_PORT || '6379'),
      retryDelayOnFailover: 100,
      maxRetriesPerRequest: 3
    });
  }

  async getTransferVolume(tokenAddress: string, timeRange: string): Promise<BigInt | null> {
    const key = `transfer_volume:${tokenAddress}:${timeRange}`;
    const cached = await this.redis.get(key);
    
    if (cached) {
      return BigInt(cached);
    }

    return null;
  }

  async setTransferVolume(tokenAddress: string, timeRange: string, volume: BigInt): Promise<void> {
    const key = `transfer_volume:${tokenAddress}:${timeRange}`;
    await this.redis.setex(key, 300, volume.toString()); // 5 minutes TTL
  }

  async invalidateCache(pattern: string): Promise<void> {
    const keys = await this.redis.keys(pattern);
    if (keys.length > 0) {
      await this.redis.del(...keys);
    }
  }
}
```

### 3. Batch Processing

```typescript
// BatchProcessor.ts
export class BatchProcessor {
  private batchSize = 1000;
  private batchTimeout = 5000; // 5 seconds
  private eventBuffer: ParsedEvent[] = [];
  private batchTimer: NodeJS.Timeout | null = null;

  addEvent(event: ParsedEvent): void {
    this.eventBuffer.push(event);

    if (this.eventBuffer.length >= this.batchSize) {
      this.processBatch();
    } else if (!this.batchTimer) {
      this.batchTimer = setTimeout(() => this.processBatch(), this.batchTimeout);
    }
  }

  private async processBatch(): Promise<void> {
    if (this.eventBuffer.length === 0) return;

    const batch = [...this.eventBuffer];
    this.eventBuffer = [];

    if (this.batchTimer) {
      clearTimeout(this.batchTimer);
      this.batchTimer = null;
    }

    try {
      await this.storeBatch(batch);
    } catch (error) {
      console.error('Error processing batch:', error);
      // Implement retry logic
    }
  }

  private async storeBatch(events: ParsedEvent[]): Promise<void> {
    const client = await this.pool.connect();
    
    try {
      await client.query('BEGIN');
      
      // Use COPY for bulk insert
      await this.copyEvents(client, events);
      
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
}
```

---

## Monitoring and Alerting

### 1. Metrics Collection

#### Prometheus Metrics

```typescript
// Metrics.ts
import { register, Counter, Histogram, Gauge } from 'prom-client';

export const eventProcessingMetrics = {
  eventsProcessed: new Counter({
    name: 'stellar_events_processed_total',
    help: 'Total number of events processed',
    labelNames: ['topic', 'contract_address']
  }),

  eventProcessingDuration: new Histogram({
    name: 'stellar_event_processing_duration_seconds',
    help: 'Time spent processing events',
    labelNames: ['topic'],
    buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]
  }),

  activeConnections: new Gauge({
    name: 'stellar_indexer_active_connections',
    help: 'Number of active WebSocket connections'
  }),

  databaseQueueSize: new Gauge({
    name: 'stellar_indexer_database_queue_size',
    help: 'Number of events waiting to be stored'
  })
};
```

### 2. Health Checks

```typescript
// HealthCheck.ts
export class HealthCheck {
  async check(): Promise<HealthStatus> {
    const checks = await Promise.allSettled([
      this.checkDatabase(),
      this.checkRedis(),
      this.checkKafka(),
      this.checkStellarRPC()
    ]);

    return {
      status: this.getOverallStatus(checks),
      checks: {
        database: checks[0],
        redis: checks[1],
        kafka: checks[2],
        stellar: checks[3]
      },
      timestamp: new Date().toISOString()
    };
  }

  private async checkDatabase(): Promise<CheckResult> {
    try {
      const result = await this.pool.query('SELECT 1');
      return { status: 'healthy', latency: result.duration };
    } catch (error) {
      return { status: 'unhealthy', error: error.message };
    }
  }

  private getOverallStatus(checks: PromiseSettledResult<CheckResult>[]): string {
    const failed = checks.filter(check => check.status === 'rejected' || 
      (check.status === 'fulfilled' && check.value.status !== 'healthy')).length;
    
    return failed === 0 ? 'healthy' : failed < checks.length ? 'degraded' : 'unhealthy';
  }
}
```

---

## Deployment Guide

### 1. Docker Configuration

```dockerfile
# Dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 3000

CMD ["npm", "start"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  indexer:
    build: .
    environment:
      - NODE_ENV=production
      - DB_HOST=postgres
      - REDIS_HOST=redis
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - postgres
      - redis
      - kafka
    ports:
      - "3000:3000"

  postgres:
    image: timescale/timescaledb:latest-pg14
    environment:
      - POSTGRES_DB=stellar_events
      - POSTGRES_USER=indexer
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

volumes:
  postgres_data:
```

### 2. Kubernetes Deployment

```yaml
# k8s-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stellar-indexer
spec:
  replicas: 3
  selector:
    matchLabels:
      app: stellar-indexer
  template:
    metadata:
      labels:
        app: stellar-indexer
    spec:
      containers:
      - name: indexer
        image: stellar-indexer:latest
        ports:
        - containerPort: 3000
        env:
        - name: DB_HOST
          value: postgres-service
        - name: REDIS_HOST
          value: redis-service
        - name: KAFKA_BROKERS
          value: kafka-service:9092
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: stellar-indexer-service
spec:
  selector:
    app: stellar-indexer
  ports:
  - port: 80
    targetPort: 3000
  type: LoadBalancer
```

---

## Security Considerations

### 1. API Security

```typescript
// SecurityMiddleware.ts
import rateLimit from 'express-rate-limit';
import helmet from 'helmet';

export const securityMiddleware = [
  helmet(),
  rateLimit({
    windowMs: 15 * 60 * 1000, // 15 minutes
    max: 100, // limit each IP to 100 requests per windowMs
    message: 'Too many requests from this IP'
  })
];
```

### 2. Data Validation

```typescript
// EventValidator.ts
export class EventValidator {
  static validate(event: any): boolean {
    if (!event.topic || !event.contractAddress || !event.timestamp) {
      return false;
    }

    if (!VALID_TOPICS.includes(event.topic)) {
      return false;
    }

    if (!this.isValidAddress(event.contractAddress)) {
      return false;
    }

    return this.validateEventData(event.topic, event.data);
  }

  private static validateEventData(topic: string, data: any): boolean {
    const schema = EVENT_SCHEMAS[topic];
    return this.validateAgainstSchema(data, schema);
  }
}
```

---

## Testing Strategy

### 1. Unit Tests

```typescript
// EventProcessor.test.ts
import { EventProcessor } from './EventProcessor';

describe('EventProcessor', () => {
  let processor: EventProcessor;
  let mockEventStore: jest.Mocked<EventStore>;

  beforeEach(() => {
    mockEventStore = {
      store: jest.fn(),
      getEvents: jest.fn()
    } as any;
    
    processor = new EventProcessor(mockEventStore);
  });

  test('should process transfer event correctly', async () => {
    const transferEvent = {
      topic: 'transfer',
      contractAddress: 'G...',
      data: {
        from: 'G...',
        to: 'G...',
        amount: '1000000',
        token: 'G...'
      }
    };

    await processor.processEvent(transferEvent);

    expect(mockEventStore.store).toHaveBeenCalledWith(
      expect.objectContaining({
        topic: 'transfer',
        contractAddress: 'G...'
      })
    );
  });
});
```

### 2. Integration Tests

```typescript
// integration.test.ts
import { setupTestDatabase } from './testUtils';

describe('Event Indexing Integration', () => {
  let testDb: any;

  beforeAll(async () => {
    testDb = await setupTestDatabase();
  });

  test('should store and retrieve events', async () => {
    const eventProcessor = new EventProcessor(testDb);
    
    await eventProcessor.processEvent(mockTransferEvent);
    
    const storedEvents = await testDb.query('SELECT * FROM events');
    expect(storedEvents.rows).toHaveLength(1);
    expect(storedEvents.rows[0].topic).toBe('transfer');
  });
});
```

---

## Troubleshooting Guide

### Common Issues

1. **Event Processing Lag**
   - Check Kafka consumer lag
   - Monitor database performance
   - Verify batch processing efficiency

2. **Memory Usage High**
   - Check for memory leaks in event processing
   - Optimize batch sizes
   - Monitor garbage collection

3. **Database Connection Issues**
   - Verify connection pool settings
   - Check database server capacity
   - Monitor connection counts

### Debugging Tools

```typescript
// DebugLogger.ts
export class DebugLogger {
  static logEventProcessing(event: ParsedEvent, duration: number) {
    console.log({
      type: 'event_processing',
      eventId: event.id,
      topic: event.topic,
      duration: `${duration}ms`,
      timestamp: new Date().toISOString()
    });
  }

  static logDatabaseQuery(query: string, params: any[], duration: number) {
    console.log({
      type: 'database_query',
      query: query.substring(0, 100) + '...',
      paramCount: params.length,
      duration: `${duration}ms`,
      timestamp: new Date().toISOString()
    });
  }
}
```

---

## Performance Benchmarks

### Expected Performance Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Event Processing Latency | < 100ms | p95 |
| Database Query Response | < 50ms | p95 |
| API Response Time | < 200ms | p95 |
| WebSocket Message Latency | < 10ms | p95 |
| Throughput | 10,000 events/sec | sustained |

### Load Testing Script

```typescript
// loadTest.ts
import { EventProcessor } from './EventProcessor';

async function runLoadTest() {
  const processor = new EventProcessor();
  const startTime = Date.now();
  const eventCount = 10000;

  for (let i = 0; i < eventCount; i++) {
    await processor.processEvent(generateMockEvent());
  }

  const duration = Date.now() - startTime;
  const eventsPerSecond = (eventCount / duration) * 1000;

  console.log(`Processed ${eventCount} events in ${duration}ms`);
  console.log(`Events per second: ${eventsPerSecond.toFixed(2)}`);
}
```

---

This comprehensive guide provides everything needed to build a robust, scalable event indexing infrastructure for Stellara Network contracts. The architecture is designed to handle high throughput while maintaining data consistency and providing real-time access to indexed events.
